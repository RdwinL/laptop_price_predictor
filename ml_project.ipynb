{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Laptop Price Prediction - Machine Learning Project\n",
    "\n",
    "This notebook implements a complete machine learning workflow to predict laptop prices using Linear Regression and Decision Tree models.\n",
    "\n",
    "## Table of Contents\n",
    "1. Data Loading and Exploration\n",
    "2. Data Preprocessing\n",
    "3. Model Training (Linear Regression & Decision Tree)\n",
    "4. Model Evaluation and Comparison\n",
    "5. Visualization\n",
    "6. Save Best Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
    "import pickle\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set visualization style\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette('husl')\n",
    "\n",
    "print('All libraries imported successfully!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data Loading and Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "df = pd.read_csv('laptop_prices.csv')\n",
    "\n",
    "print(f\"Dataset Shape: {df.shape}\")\n",
    "print(f\"\\nNumber of rows: {df.shape[0]}\")\n",
    "print(f\"Number of columns: {df.shape[1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display first few rows\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset information\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Statistical summary\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for missing values\n",
    "print(\"Missing Values:\")\n",
    "print(df.isnull().sum())\n",
    "print(f\"\\nTotal missing values: {df.isnull().sum().sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Target variable distribution\n",
    "plt.figure(figsize=(12, 4))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.hist(df['Price_Tsh'], bins=50, edgecolor='black', alpha=0.7)\n",
    "plt.xlabel('Price (Tsh)', fontsize=12)\n",
    "plt.ylabel('Frequency', fontsize=12)\n",
    "plt.title('Distribution of Laptop Prices', fontsize=14, fontweight='bold')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.boxplot(df['Price_Tsh'])\n",
    "plt.ylabel('Price (Tsh)', fontsize=12)\n",
    "plt.title('Boxplot of Laptop Prices', fontsize=14, fontweight='bold')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('price_distribution.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"Mean Price: {df['Price_Tsh'].mean():,.2f} Tsh\")\n",
    "print(f\"Median Price: {df['Price_Tsh'].median():,.2f} Tsh\")\n",
    "print(f\"Min Price: {df['Price_Tsh'].min():,.2f} Tsh\")\n",
    "print(f\"Max Price: {df['Price_Tsh'].max():,.2f} Tsh\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a copy for preprocessing\n",
    "df_processed = df.copy()\n",
    "\n",
    "print(\"Original dataset shape:\", df_processed.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select important features for modeling\n",
    "# We'll focus on the most relevant features\n",
    "\n",
    "selected_features = [\n",
    "    'Company', 'TypeName', 'Inches', 'Ram', 'OS', 'Weight',\n",
    "    'Touchscreen', 'IPSpanel', 'RetinaDisplay', 'CPU_company',\n",
    "    'CPU_freq', 'PrimaryStorage', 'GPU_company', 'Price_Tsh'\n",
    "]\n",
    "\n",
    "df_processed = df_processed[selected_features]\n",
    "print(\"Selected features:\", df_processed.shape[1] - 1)  # -1 for target variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Handle missing values\n",
    "print(\"Missing values before handling:\")\n",
    "print(df_processed.isnull().sum())\n",
    "print()\n",
    "\n",
    "# Drop rows with missing values (if any)\n",
    "df_processed = df_processed.dropna()\n",
    "print(f\"Dataset shape after handling missing values: {df_processed.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert categorical Yes/No to binary\n",
    "binary_columns = ['Touchscreen', 'IPSpanel', 'RetinaDisplay']\n",
    "\n",
    "for col in binary_columns:\n",
    "    df_processed[col] = (df_processed[col] == 'Yes').astype(int)\n",
    "\n",
    "print(\"Binary columns converted successfully\")\n",
    "print(df_processed[binary_columns].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode categorical variables\n",
    "categorical_columns = ['Company', 'TypeName', 'OS', 'CPU_company', 'GPU_company']\n",
    "\n",
    "# Create label encoders dictionary to save for later use\n",
    "label_encoders = {}\n",
    "\n",
    "for col in categorical_columns:\n",
    "    le = LabelEncoder()\n",
    "    df_processed[col] = le.fit_transform(df_processed[col])\n",
    "    label_encoders[col] = le\n",
    "    print(f\"{col}: {len(le.classes_)} unique values\")\n",
    "\n",
    "print(\"\\nCategorical encoding completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for any remaining non-numeric data\n",
    "print(\"Data types after preprocessing:\")\n",
    "print(df_processed.dtypes)\n",
    "print(\"\\nProcessed dataset:\")\n",
    "df_processed.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation analysis\n",
    "plt.figure(figsize=(12, 10))\n",
    "correlation_matrix = df_processed.corr()\n",
    "sns.heatmap(correlation_matrix, annot=True, fmt='.2f', cmap='coolwarm', \n",
    "            center=0, square=True, linewidths=1)\n",
    "plt.title('Feature Correlation Heatmap', fontsize=16, fontweight='bold', pad=20)\n",
    "plt.tight_layout()\n",
    "plt.savefig('correlation_heatmap.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nTop correlations with Price:\")\n",
    "price_corr = correlation_matrix['Price_Tsh'].sort_values(ascending=False)\n",
    "print(price_corr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split features and target\n",
    "X = df_processed.drop('Price_Tsh', axis=1)\n",
    "y = df_processed['Price_Tsh']\n",
    "\n",
    "print(f\"Features shape: {X.shape}\")\n",
    "print(f\"Target shape: {y.shape}\")\n",
    "print(f\"\\nFeature names: {list(X.columns)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into training and testing sets (80-20 split)\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "print(f\"Training set size: {X_train.shape[0]} samples\")\n",
    "print(f\"Testing set size: {X_test.shape[0]} samples\")\n",
    "print(f\"\\nTraining set: {X_train.shape[0]/len(X)*100:.1f}%\")\n",
    "print(f\"Testing set: {X_test.shape[0]/len(X)*100:.1f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature scaling\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "print(\"Feature scaling completed!\")\n",
    "print(f\"\\nScaled training data shape: {X_train_scaled.shape}\")\n",
    "print(f\"Scaled testing data shape: {X_test_scaled.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Model Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Linear Regression Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Linear Regression model\n",
    "print(\"Training Linear Regression Model...\")\n",
    "lr_model = LinearRegression()\n",
    "lr_model.fit(X_train_scaled, y_train)\n",
    "print(\"Linear Regression model trained successfully!\")\n",
    "\n",
    "# Make predictions\n",
    "y_pred_lr_train = lr_model.predict(X_train_scaled)\n",
    "y_pred_lr_test = lr_model.predict(X_test_scaled)\n",
    "\n",
    "print(\"\\nPredictions completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Decision Tree Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Decision Tree model\n",
    "print(\"Training Decision Tree Model...\")\n",
    "dt_model = DecisionTreeRegressor(random_state=42, max_depth=10)\n",
    "dt_model.fit(X_train_scaled, y_train)\n",
    "print(\"Decision Tree model trained successfully!\")\n",
    "\n",
    "# Make predictions\n",
    "y_pred_dt_train = dt_model.predict(X_train_scaled)\n",
    "y_pred_dt_test = dt_model.predict(X_test_scaled)\n",
    "\n",
    "print(\"\\nPredictions completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Model Evaluation and Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to calculate evaluation metrics\n",
    "def evaluate_model(y_true, y_pred, model_name, dataset_type):\n",
    "    mse = mean_squared_error(y_true, y_pred)\n",
    "    rmse = np.sqrt(mse)\n",
    "    mae = mean_absolute_error(y_true, y_pred)\n",
    "    r2 = r2_score(y_true, y_pred)\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"{model_name} - {dataset_type} Set Performance\")\n",
    "    print(f\"{'='*60}\")\n",
    "    print(f\"Mean Squared Error (MSE):     {mse:,.2f}\")\n",
    "    print(f\"Root Mean Squared Error (RMSE): {rmse:,.2f}\")\n",
    "    print(f\"Mean Absolute Error (MAE):     {mae:,.2f}\")\n",
    "    print(f\"R² Score:                      {r2:.4f}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    return {'MSE': mse, 'RMSE': rmse, 'MAE': mae, 'R2': r2}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate Linear Regression\n",
    "lr_train_metrics = evaluate_model(y_train, y_pred_lr_train, \"Linear Regression\", \"Training\")\n",
    "lr_test_metrics = evaluate_model(y_test, y_pred_lr_test, \"Linear Regression\", \"Testing\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate Decision Tree\n",
    "dt_train_metrics = evaluate_model(y_train, y_pred_dt_train, \"Decision Tree\", \"Training\")\n",
    "dt_test_metrics = evaluate_model(y_test, y_pred_dt_test, \"Decision Tree\", \"Testing\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comparison dataframe\n",
    "comparison_df = pd.DataFrame({\n",
    "    'Model': ['Linear Regression', 'Decision Tree'],\n",
    "    'Train_RMSE': [lr_train_metrics['RMSE'], dt_train_metrics['RMSE']],\n",
    "    'Test_RMSE': [lr_test_metrics['RMSE'], dt_test_metrics['RMSE']],\n",
    "    'Train_MAE': [lr_train_metrics['MAE'], dt_train_metrics['MAE']],\n",
    "    'Test_MAE': [lr_test_metrics['MAE'], dt_test_metrics['MAE']],\n",
    "    'Train_R2': [lr_train_metrics['R2'], dt_train_metrics['R2']],\n",
    "    'Test_R2': [lr_test_metrics['R2'], dt_test_metrics['R2']]\n",
    "})\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"MODEL COMPARISON SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "print(comparison_df.to_string(index=False))\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine best model based on Test R² Score\n",
    "if lr_test_metrics['R2'] > dt_test_metrics['R2']:\n",
    "    best_model = lr_model\n",
    "    best_model_name = \"Linear Regression\"\n",
    "    best_predictions = y_pred_lr_test\n",
    "    best_r2 = lr_test_metrics['R2']\n",
    "else:\n",
    "    best_model = dt_model\n",
    "    best_model_name = \"Decision Tree\"\n",
    "    best_predictions = y_pred_dt_test\n",
    "    best_r2 = dt_test_metrics['R2']\n",
    "\n",
    "print(f\"\\n{'*'*80}\")\n",
    "print(f\"BEST MODEL: {best_model_name}\")\n",
    "print(f\"Test R² Score: {best_r2:.4f}\")\n",
    "print(f\"{'*'*80}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model comparison visualization\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "\n",
    "# 1. RMSE Comparison\n",
    "models = ['Linear Regression', 'Decision Tree']\n",
    "train_rmse = [lr_train_metrics['RMSE'], dt_train_metrics['RMSE']]\n",
    "test_rmse = [lr_test_metrics['RMSE'], dt_test_metrics['RMSE']]\n",
    "\n",
    "x = np.arange(len(models))\n",
    "width = 0.35\n",
    "\n",
    "axes[0, 0].bar(x - width/2, train_rmse, width, label='Training', alpha=0.8)\n",
    "axes[0, 0].bar(x + width/2, test_rmse, width, label='Testing', alpha=0.8)\n",
    "axes[0, 0].set_xlabel('Model', fontsize=12, fontweight='bold')\n",
    "axes[0, 0].set_ylabel('RMSE', fontsize=12, fontweight='bold')\n",
    "axes[0, 0].set_title('RMSE Comparison', fontsize=14, fontweight='bold')\n",
    "axes[0, 0].set_xticks(x)\n",
    "axes[0, 0].set_xticklabels(models)\n",
    "axes[0, 0].legend()\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# 2. R² Score Comparison\n",
    "train_r2 = [lr_train_metrics['R2'], dt_train_metrics['R2']]\n",
    "test_r2 = [lr_test_metrics['R2'], dt_test_metrics['R2']]\n",
    "\n",
    "axes[0, 1].bar(x - width/2, train_r2, width, label='Training', alpha=0.8)\n",
    "axes[0, 1].bar(x + width/2, test_r2, width, label='Testing', alpha=0.8)\n",
    "axes[0, 1].set_xlabel('Model', fontsize=12, fontweight='bold')\n",
    "axes[0, 1].set_ylabel('R² Score', fontsize=12, fontweight='bold')\n",
    "axes[0, 1].set_title('R² Score Comparison', fontsize=14, fontweight='bold')\n",
    "axes[0, 1].set_xticks(x)\n",
    "axes[0, 1].set_xticklabels(models)\n",
    "axes[0, 1].legend()\n",
    "axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# 3. Actual vs Predicted - Linear Regression\n",
    "axes[1, 0].scatter(y_test, y_pred_lr_test, alpha=0.6, s=50)\n",
    "axes[1, 0].plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], \n",
    "                'r--', lw=2, label='Perfect Prediction')\n",
    "axes[1, 0].set_xlabel('Actual Price (Tsh)', fontsize=12, fontweight='bold')\n",
    "axes[1, 0].set_ylabel('Predicted Price (Tsh)', fontsize=12, fontweight='bold')\n",
    "axes[1, 0].set_title(f'Linear Regression: Actual vs Predicted\\nR² = {lr_test_metrics[\"R2\"]:.4f}', \n",
    "                     fontsize=14, fontweight='bold')\n",
    "axes[1, 0].legend()\n",
    "axes[1, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# 4. Actual vs Predicted - Decision Tree\n",
    "axes[1, 1].scatter(y_test, y_pred_dt_test, alpha=0.6, s=50, color='orange')\n",
    "axes[1, 1].plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], \n",
    "                'r--', lw=2, label='Perfect Prediction')\n",
    "axes[1, 1].set_xlabel('Actual Price (Tsh)', fontsize=12, fontweight='bold')\n",
    "axes[1, 1].set_ylabel('Predicted Price (Tsh)', fontsize=12, fontweight='bold')\n",
    "axes[1, 1].set_title(f'Decision Tree: Actual vs Predicted\\nR² = {dt_test_metrics[\"R2\"]:.4f}', \n",
    "                     fontsize=14, fontweight='bold')\n",
    "axes[1, 1].legend()\n",
    "axes[1, 1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('model_comparison.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Residual analysis for both models\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "# Linear Regression residuals\n",
    "residuals_lr = y_test - y_pred_lr_test\n",
    "axes[0].scatter(y_pred_lr_test, residuals_lr, alpha=0.6)\n",
    "axes[0].axhline(y=0, color='r', linestyle='--', linewidth=2)\n",
    "axes[0].set_xlabel('Predicted Price (Tsh)', fontsize=12, fontweight='bold')\n",
    "axes[0].set_ylabel('Residuals', fontsize=12, fontweight='bold')\n",
    "axes[0].set_title('Linear Regression: Residual Plot', fontsize=14, fontweight='bold')\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Decision Tree residuals\n",
    "residuals_dt = y_test - y_pred_dt_test\n",
    "axes[1].scatter(y_pred_dt_test, residuals_dt, alpha=0.6, color='orange')\n",
    "axes[1].axhline(y=0, color='r', linestyle='--', linewidth=2)\n",
    "axes[1].set_xlabel('Predicted Price (Tsh)', fontsize=12, fontweight='bold')\n",
    "axes[1].set_ylabel('Residuals', fontsize=12, fontweight='bold')\n",
    "axes[1].set_title('Decision Tree: Residual Plot', fontsize=14, fontweight='bold')\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('residual_plots.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature importance (for Decision Tree)\n",
    "feature_importance = pd.DataFrame({\n",
    "    'Feature': X.columns,\n",
    "    'Importance': dt_model.feature_importances_\n",
    "}).sort_values('Importance', ascending=False)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.barh(feature_importance['Feature'], feature_importance['Importance'])\n",
    "plt.xlabel('Importance', fontsize=12, fontweight='bold')\n",
    "plt.ylabel('Feature', fontsize=12, fontweight='bold')\n",
    "plt.title('Decision Tree: Feature Importance', fontsize=14, fontweight='bold')\n",
    "plt.gca().invert_yaxis()\n",
    "plt.grid(True, alpha=0.3, axis='x')\n",
    "plt.tight_layout()\n",
    "plt.savefig('feature_importance.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nTop 5 Most Important Features:\")\n",
    "print(feature_importance.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Save Best Model and Preprocessing Objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the best model\n",
    "model_data = {\n",
    "    'model': best_model,\n",
    "    'scaler': scaler,\n",
    "    'label_encoders': label_encoders,\n",
    "    'feature_names': list(X.columns),\n",
    "    'model_name': best_model_name,\n",
    "    'test_r2_score': best_r2,\n",
    "    'test_rmse': lr_test_metrics['RMSE'] if best_model_name == 'Linear Regression' else dt_test_metrics['RMSE']\n",
    "}\n",
    "\n",
    "with open('model.pkl', 'wb') as f:\n",
    "    pickle.dump(model_data, f)\n",
    "\n",
    "print(f\"✓ Best model ({best_model_name}) saved successfully as 'model.pkl'\")\n",
    "print(f\"✓ Model R² Score: {best_r2:.4f}\")\n",
    "print(f\"\\nThe model file includes:\")\n",
    "print(\"  - Trained model\")\n",
    "print(\"  - Feature scaler\")\n",
    "print(\"  - Label encoders\")\n",
    "print(\"  - Feature names\")\n",
    "print(\"  - Model metadata\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Model Testing with Sample Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the saved model with sample predictions\n",
    "print(\"Testing the saved model with sample predictions...\\n\")\n",
    "\n",
    "# Load the model\n",
    "with open('model.pkl', 'rb') as f:\n",
    "    loaded_model_data = pickle.load(f)\n",
    "\n",
    "# Get 5 random samples from test set\n",
    "sample_indices = np.random.choice(X_test.index, 5, replace=False)\n",
    "samples = X_test.loc[sample_indices]\n",
    "actual_prices = y_test.loc[sample_indices]\n",
    "\n",
    "# Make predictions\n",
    "samples_scaled = loaded_model_data['scaler'].transform(samples)\n",
    "predictions = loaded_model_data['model'].predict(samples_scaled)\n",
    "\n",
    "# Display results\n",
    "results_df = pd.DataFrame({\n",
    "    'Actual Price (Tsh)': actual_prices.values,\n",
    "    'Predicted Price (Tsh)': predictions,\n",
    "    'Difference (Tsh)': actual_prices.values - predictions,\n",
    "    'Error (%)': np.abs((actual_prices.values - predictions) / actual_prices.values * 100)\n",
    "})\n",
    "\n",
    "print(\"Sample Predictions:\")\n",
    "print(\"=\"*80)\n",
    "print(results_df.to_string(index=False))\n",
    "print(\"=\"*80)\n",
    "print(f\"\\nMean Absolute Error: {np.abs(results_df['Difference (Tsh)']).mean():,.2f} Tsh\")\n",
    "print(f\"Mean Percentage Error: {results_df['Error (%)'].mean():.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook successfully completed the following tasks:\n",
    "\n",
    "1. **Data Loading and Exploration**: Loaded and analyzed the laptop prices dataset\n",
    "2. **Data Preprocessing**: \n",
    "   - Handled missing values\n",
    "   - Encoded categorical variables\n",
    "   - Scaled features\n",
    "   - Split data into training and testing sets\n",
    "3. **Model Training**: Trained both Linear Regression and Decision Tree models\n",
    "4. **Model Evaluation**: Evaluated and compared both models using multiple metrics\n",
    "5. **Visualization**: Created comprehensive visualizations for model comparison\n",
    "6. **Model Saving**: Saved the best-performing model for deployment\n",
    "\n",
    "The saved model is ready for deployment in the AI application!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
